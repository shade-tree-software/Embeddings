import pdb
import pickle
import string
import sys
from os import listdir
from os.path import isfile, join
import numpy as np
import json

from vertexai.preview.language_models import TextEmbeddingModel

from utils import (cosine_similarity, process_text)

# Match query text with the document that has the closest embedding.
#
# Documents are read from a JSONL file.  If the file also contains embeddings
# for each document, then they are assumed to be Google embeddings and the
# query text embedding is requested from Google.  If the file contains no
# embeddings then all embeddings are generated locally.
#
# Local embeddings are created by summing embeddings for each word, in which
# case any context based on word order is lost.  Thus, local embeddings may
# not be as accurate as embeddings generated by Google.
#
# If a JSONL file is not specified, documents and embeddings are loaded from
# pickle files.

DOCS_PICKLE = "data/docs.p"
DOC_VECS_PICKLE = "data/docVecs.p"
EMBEDDINGS_PICKLE = "data/glove.6B.300d.p"
# EMBEDDINGS_PICKLE = "data/local_embeddings.p"

local_embeddings = None
docs = None
doc_vecs = None
docs_jsonl = None
query = sys.argv[1]
if (len(sys.argv) == 3):
    docs_jsonl = sys.argv[2]
else:
    docs = pickle.load(open(DOCS_PICKLE, "rb"))
    doc_vecs = pickle.load(open(DOC_VECS_PICKLE, "rb"))

def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):
    """
    Input:
      - v, the vector you are going find the nearest neighbor for
      - candidates: a set of vectors where we will find the neighbors
      - k: top k nearest neighbors to find
    Output:
      - the indices of the top k closest vectors
    """
    cos_similarities = []
    # get cosine similarity of input vector v and each candidate vector
    for candidate in candidates:
        cos_similarities.append(cosine_similarity(v, candidate))
    # sort the similarity list and get the k most similar indices    
    return np.flip(np.argsort(cos_similarities))[:k]

def get_document_embedding_verbose(text, embeddings, process_text=process_text):
    '''
    Input:
        - text: a string
        - embeddings: a dictionary of word embeddings
    Output:
        - doc_embedding: sum of all word embeddings in the tweet
    '''
    doc_embedding = np.zeros(300)
    # process the document into a list of words (process the tweet)
    processed_doc = process_text(text)
    words_with_embeddings = set()
    for word in processed_doc:
        # add the word embedding to the running total for the document embedding
        word_embedding = embeddings.get(word, 0)
        if isinstance(word_embedding, np.ndarray):
            words_with_embeddings.add(word)
        doc_embedding += word_embedding
    return doc_embedding, words_with_embeddings, processed_doc

def get_document_embedding(text, embeddings, process_text=process_text):
    return get_document_embedding_verbose(text, embeddings, process_text)[0]

def get_document_vecs(all_docs, embeddings, get_document_embedding=get_document_embedding):
    '''
    Input:
        - all_docs: list of strings - all documents in our dataset.
        - embeddings: dictionary with words as the keys and their embeddings as the values.
    Output:
        - document_vec_matrix: matrix of document embeddings.
        - ind2Doc_dict: dictionary with indices of docs in vecs as keys and their embeddings as the values.
    '''

    # the dictionary's key is an index (integer) that identifies a specific document
    # the value is the document embedding for that document
    ind2Doc_dict = {}

    # this is list that will store the document vectors
    document_vec_l = []

    for i, doc in enumerate(all_docs):
        doc_embedding = get_document_embedding(doc, embeddings)
        ind2Doc_dict[i] = doc_embedding
        document_vec_l.append(doc_embedding)

    # convert the list of document vectors into a 2D array (each row is a document vector)
    document_vec_matrix = np.vstack(document_vec_l)

    return {"type":"local", "values":document_vec_matrix}, ind2Doc_dict

# load documents and embeddings.  If no embeddings, generate them locally
if not docs or not doc_vecs:
    with open(docs_jsonl, "r") as f:
        docs_info = [json.loads(doc_info) for doc_info in f.readlines()]
    if "instance" in docs_info[0] and "predictions" in docs_info[0]:
        docs = []
        doc_vecs_l = []
        for doc_info in docs_info:
            docs.append(doc_info["instance"]["content"])
            doc_vecs_l.append(doc_info["predictions"][0]["embeddings"]["values"])
        doc_vecs = {"type":"Google", "values":np.vstack(doc_vecs_l)}
        print(f"Loaded {len(docs)} docs and embeddings from {docs_jsonl}")
    elif "content" in docs_info[0]:
        docs = [doc_info["content"] for doc_info in docs_info]
        print(f"Loaded {len(docs)} docs from {docs_jsonl}.  No embeddings found.  Generating all embeddings locally.")
        if not local_embeddings:
            local_embeddings = pickle.load(open(EMBEDDINGS_PICKLE, "rb"))
        doc_vecs, _ = get_document_vecs(docs, local_embeddings)
    else:
        print("could not read input file")
        exit(0)

# generate embeddings for query text
if doc_vecs["type"] == "local":
    if not local_embeddings:
        local_embeddings = pickle.load(open(EMBEDDINGS_PICKLE, "rb"))
    query_embedding, words_with_embeddings, _ = get_document_embedding_verbose(query, local_embeddings)
    print(f"Query words: {words_with_embeddings}")
elif doc_vecs["type"] == "Google":
    print("Requesting query text embedding from Google")
    model = TextEmbeddingModel.from_pretrained("textembedding-gecko")
    query_embedding = model.get_embeddings([query])[0].values
else:
    print("unknown embeddings type")
    exit(0)

# print best match
idx = np.argmax(cosine_similarity(doc_vecs["values"], query_embedding))
print(f"Best document match for query text:\n{docs[idx]}")
if local_embeddings:
    print("Words from document that have embeddings:")
    print(get_document_embedding_verbose(docs[idx], local_embeddings)[1])
else:
    print("Cleaned words from document:")
    print(set(process_text(docs[idx])))

if docs_jsonl:
    print(f"Generating docs and embeddings pickle files")
    pickle.dump(docs, open(DOCS_PICKLE, "wb"))
    pickle.dump(doc_vecs, open(DOC_VECS_PICKLE, "wb"))

