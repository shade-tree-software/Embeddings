import pdb
import pickle
import string
import sys
from os import listdir
from os.path import isfile, join
import numpy as np
import json

from utils import (cosine_similarity, process_text)

# Match query text with the document that has the closest embedding.
#
# Documents are read from a JSONL file.  If the file also contains embeddings
# for each document, then they are assumed to be Google embeddings and the
# query text embedding is requested from Google.  If the file contains no
# embeddings then all embeddings are generated locally.
#
# Local embeddings are created by summing embeddings for each word, in which
# case any context based on word order is lost.  Thus, local embeddings may
# not be as accurate as embeddings generated by Google.
#
# If a JSONL file is not specified, documents and embeddings are loaded from
# pickle files.

LOCAL_DOCS_PICKLE = "data/docs_local.p"
GOOGLE_DOCS_PICKLE = "data/docs_google.p"
LOCAL_DOC_VECS_PICKLE = "data/docVecs_local.p"
GOOGLE_DOC_VECS_PICKLE = "data/docVecs_google.p"
LOCAL_WORD_VECS_PICKLE = "data/glove.840B.300d.p"
# LOCAL_WORD_VECS_PICKLE = "data/glove.6B.300d.p"
# LOCAL_WORD_VECS_PICKLE = "data/local_word_vecs.p"
GOOGLE_ANN_DATA_PICKLE = "data/annDatNotImplementedErrora_google.p"
LOCAL_ANN_DATA_PICKLE = "data/annData_local.p"
N_VECS_PER_BUCKET = 16
N_UNIVERSES = 25

def nearest_neighbor(query_vec, document_vectors, k=1, cosine_similarity=cosine_similarity):
    """
    Input:
      - query_vec, the vector you are going find the nearest neighbor for
      - document_vectors: a set of vectors where we will find the neighbors
      - k: top k nearest neighbors to find
    Output:
      - the top k closest vectors
    """
    cos_similarities = []
    # get cosine similarity of input vector v and each candidate vector
    for doc_vec in document_vectors:
        cos_similarities.append(cosine_similarity(np.array(query_vec), np.array(doc_vec)))
    # sort the similarity list and get the k most similar indices    
    return np.flip(np.argsort(cos_similarities))[:k]

def get_document_embedding_verbose(text, embeddings, process_text=process_text):
    '''
    Input:
        - text: a string
        - embeddings: a dictionary of word embeddings
    Output:
        - doc_embedding: sum of all word embeddings in the tweet
    '''
    doc_embedding = np.zeros(300)
    # process the document into a list of words (process the tweet)
    processed_doc = process_text(text)
    words_with_embeddings = set()
    for word in processed_doc:
        # add the word embedding to the running total for the document embedding
        word_embedding = embeddings.get(word, 0)
        if isinstance(word_embedding, np.ndarray):
            words_with_embeddings.add(word)
        doc_embedding += word_embedding
    return doc_embedding, words_with_embeddings, processed_doc

def get_document_embedding(text, embeddings, process_text=process_text):
    return get_document_embedding_verbose(text, embeddings, process_text)[0]

def get_document_vecs(all_docs, embeddings, get_document_embedding=get_document_embedding):
    '''
    Input:
        - all_docs: list of strings - all documents in our dataset.
        - embeddings: dictionary with words as the keys and their embeddings as the values.
    Output:
        - document_vec_matrix: matrix of document embeddings.
        - ind2Doc_dict: dictionary with indices of docs in vecs as keys and their embeddings as the values.
    '''

    # the dictionary's key is an index (integer) that identifies a specific document
    # the value is the document embedding for that document
    ind2Doc_dict = {}

    # this is list that will store the document vectors
    document_vec_l = []

    for i, doc in enumerate(all_docs):
        doc_embedding = get_document_embedding(doc, embeddings)
        ind2Doc_dict[i] = doc_embedding
        document_vec_l.append(doc_embedding)

    # convert the list of document vectors into a 2D array (each row is a document vector)
    document_vec_matrix = np.vstack(document_vec_l)

    return document_vec_matrix, ind2Doc_dict

def hash_value_of_vector(v, planes):
    """Create a hash for a vector based on whether the vector falls on the positive
       or negative side of each plane.
    Input:
        - v:  vector of document. Its dimension is (1, N_DIMS)
        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region
    Output:
        - returns: hash of vector

    """
    # for the set of planes,
    # calculate the dot product between the vector and the matrix containing the planes
    # The dot product will have the shape (1, N_PLANES)    
    dot_product = np.dot(v, planes)

    # Return an integer where each bit represents whether the dot product in the
    # corresponding plane was positive or negative.
    return int((2 ** (np.where(dot_product.squeeze() >= 0)[0])).sum())

def make_hash_table(vecs, planes, hash_value_of_vector=hash_value_of_vector):
    """
    Input:
        - vecs: list of vectors to be hashed.
        - planes: the matrix of planes in a single "universe", with shape (embedding dimensions, number of planes).
    Output:
        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)
        - id_table: dictionary - keys are hashes, values are list of vectors id's
                            (it's used to know which doc corresponds to the hashed vector)
    """
    # number of planes is the number of columns in the planes matrix
    num_of_planes = planes.shape[1]

    # number of buckets is 2^(number of planes)
    num_buckets = 2**num_of_planes

    # create the hash table as a dictionary.
    # Keys are integers (0,1,2.. number of buckets)
    # Values are empty lists
    hash_table = {i: [] for i in range(num_buckets)}

    # create the id table as a dictionary.
    # Keys are integers (0,1,2... number of buckets)
    # Values are empty lists
    id_table = {i: [] for i in range(num_buckets)}

    for i, v in enumerate(vecs):
        h = hash_value_of_vector(v, planes)

        # store the vector into hash_table at key h,
        # by appending the vector v to the list at key h
        hash_table[h].append(v) 

        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)
        # the key is the h, and the 'i' is appended to the list at key h
        id_table[h].append(i) 

    return hash_table, id_table

def create_hash_id_tables(planes_l, document_vecs):
    hash_tables = []
    id_tables = []
    n_universes = len(planes_l)
    for universe_id in range(n_universes):  # there are 25 hashes
        print('working on hash universe #:', universe_id)
        planes = planes_l[universe_id]
        hash_table, id_table = make_hash_table(document_vecs, planes)
        hash_tables.append(hash_table)
        id_tables.append(id_table)
    
    return hash_tables, id_tables

def approximate_knn(query_vec, planes_l, hash_tables, id_tables, k=1, n_docs=None,
                    num_universes_to_use=N_UNIVERSES, hash_value_of_vector=hash_value_of_vector):
    assert num_universes_to_use <= N_UNIVERSES
    vecs_to_consider_l = list() # Vectors to check for possible nearest neighbor
    ids_to_consider_l = list() # list of document IDs

    # create a set of ids to consider, for faster checking if a document ID already exists in the set
    ids_to_consider_set = set()

    for universe_id in range(num_universes_to_use):
        planes = planes_l[universe_id] # get planes for this universe
        hash_value = hash_value_of_vector(query_vec, planes) # get the bucket in which the query resides
        document_vectors_l = hash_tables[universe_id][hash_value] # get docs from this bucket
        new_ids_to_consider = id_tables[universe_id][hash_value] # get docs ids from this bucket

        # loop through the subset of document vectors to consider
        for i, new_id in enumerate(new_ids_to_consider):
            # if the document ID is not yet in the set ids_to_consider...
            if new_id not in ids_to_consider_set:
                # access document_vectors_l list at index i to get the embedding
                # then append it to the list of vectors to consider as possible nearest neighbors
                document_vector_at_i = document_vectors_l[i]
                vecs_to_consider_l.append(document_vector_at_i)

                # append the new_id (the index for the document) to the list of ids to consider
                ids_to_consider_l.append(new_id)

                # also add the new_id to the set of ids to consider
                # (use this to check if new_id is not already in the IDs to consider)
                ids_to_consider_set.add(new_id)

    # Now run k-NN on the smaller set of vecs-to-consider.
    if (n_docs):
        print(f"Fast considering {len(vecs_to_consider_l)} vecs out of {n_docs} total")
    else:
        print(f"Fast considering {len(vecs_to_consider_l)} vecs")
    nearest_neighbor_idx_l = nearest_neighbor(query_vec, vecs_to_consider_l, k=k)

    # Use the nearest neighbor index list as indices into the ids to consider
    # create a list of nearest neighbors by the document ids
    nearest_neighbor_ids = [ids_to_consider_l[idx]
                            for idx in nearest_neighbor_idx_l]

    return nearest_neighbor_ids

# Load data based on command line options
local_word_vecs = None
query = sys.argv[1]
try:
    docs_jsonl = sys.argv[sys.argv.index("-j") + 1]
except:
    docs_jsonl = None
try:
    k = int(sys.argv[sys.argv.index("-k") + 1])
except:
    k = 1
if docs_jsonl:
    with open(docs_jsonl, "r") as f:
        docs_info = [json.loads(doc_info) for doc_info in f.readlines()]
    if "instance" in docs_info[0] and "predictions" in docs_info[0]:
        docs = []
        doc_vecs_l = []
        for doc_info in docs_info:
            docs.append(doc_info["instance"]["content"])
            doc_vecs_l.append(doc_info["predictions"][0]["embeddings"]["values"])
        doc_vecs = np.vstack(doc_vecs_l)
        print(f"Loaded {len(docs)} docs and embeddings from {docs_jsonl}")
    elif "content" in docs_info[0]:
        docs = [doc_info["content"] for doc_info in docs_info]
        print(f"Loaded {len(docs)} docs from {docs_jsonl}.  No embeddings found.  Generating all embeddings locally.")
        local_word_vecs = pickle.load(open(LOCAL_WORD_VECS_PICKLE, "rb"))
        doc_vecs, _ = get_document_vecs(docs, local_word_vecs)
    else:
        print("could not read input file")
        exit(0)
else:
    if "-g" in sys.argv:
        try:
            docs = pickle.load(open(GOOGLE_DOCS_PICKLE, "rb"))
        except:
            print("No JSONL file specified and cannot find docs pickle file")
            exit(0)
        try:
            doc_vecs = pickle.load(open(GOOGLE_DOC_VECS_PICKLE, "rb"))
        except:
            print("Google option specified but cannot find Google doc embeddings pickle file")
            exit(0)
    else:
        try:
            docs = pickle.load(open(LOCAL_DOCS_PICKLE, "rb"))
        except:
            print("No JSONL file specified and cannot find docs pickle file")
            exit(0)
        try:
            doc_vecs = pickle.load(open(LOCAL_DOC_VECS_PICKLE, "rb"))
        except:
            print("No JSONL file specified and cannot find local doc embeddings pickle file")
            exit(0)
        try:
            local_word_vecs = pickle.load(open(LOCAL_WORD_VECS_PICKLE, "rb"))
        except:
            print("Cannot find local word embeddings pickle file")
            exit(0)

# generate embeddings for query text
if local_word_vecs:
    query_vec, words_with_embeddings, _ = get_document_embedding_verbose(query, local_word_vecs)
    print(f"Query words: {words_with_embeddings}")
else:
    print("Requesting query text embedding from Google")
    from vertexai.preview.language_models import TextEmbeddingModel
    model = TextEmbeddingModel.from_pretrained("textembedding-gecko")
    query_vec = model.get_embeddings([query])[0].values

# Get best match using brute force approach
# idx = np.argmax(cosine_similarity(doc_vecs, query_vec))

# Get best match using approximate nearest neighbors approach
if docs_jsonl:
    n_dims = len(query_vec)
    n_buckets = np.ceil(len(docs) / N_VECS_PER_BUCKET)
    n_planes = int(np.ceil(np.log2(n_buckets)))
    planes_l = [np.random.normal(size=(n_dims, n_planes)) for _ in range(N_UNIVERSES)]
    hash_tables, id_tables = create_hash_id_tables(planes_l, doc_vecs)
    annData = {
        "planes_l": planes_l,
        "hash_tables": hash_tables,
        "id_tables": id_tables
    }
    if local_word_vecs:
        pickle.dump(annData, open(LOCAL_ANN_DATA_PICKLE, "wb"))
    else:
        pickle.dump(annData, open(GOOGLE_ANN_DATA_PICKLE, "wb"))
else:
    if local_word_vecs:
        annData = pickle.load(open(LOCAL_ANN_DATA_PICKLE, "rb"))
    else:
        annData = pickle.load(open(GOOGLE_ANN_DATA_PICKLE, "rb"))
nearest_neighbor_ids = approximate_knn(query_vec, 
                                       annData["planes_l"],
                                       annData["hash_tables"],
                                       annData["id_tables"],
                                       k, num_universes_to_use=10, n_docs=len(docs))

print(f"Best document matches:")
for i in nearest_neighbor_ids:
    print(f"\n   {docs[i]}\n")

if docs_jsonl:
    print(f"Saving docs as pickle file")
    if local_word_vecs:
        pickle.dump(docs, open(LOCAL_DOCS_PICKLE, "wb"))
        print(f"Saving local doc embeddings as pickle file")
        pickle.dump(doc_vecs, open(LOCAL_DOC_VECS_PICKLE, "wb"))
    else:
        pickle.dump(docs, open(GOOGLE_DOCS_PICKLE, "wb"))
        print(f"Saving Google doc embeddings as pickle file")
        pickle.dump(doc_vecs, open(GOOGLE_DOC_VECS_PICKLE, "wb"))

